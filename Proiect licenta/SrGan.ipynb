{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.utils as vutils\n",
    "import os\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILS\n",
    "\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'])\n",
    "\n",
    "def calculate_valid_crop_size(crop_size, upscale_factor):\n",
    "    return crop_size - (crop_size % upscale_factor)\n",
    "\n",
    "def train_hr_transform(crop_size):\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomCrop(crop_size),\n",
    "        transforms.ToTensor()])\n",
    "\n",
    "def train_lr_transform(crop_size, upscale_factor):\n",
    "    return transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(crop_size // upscale_factor, interpolation = Image.BICUBIC),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "def display_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(400),\n",
    "        transforms.CenterCrop(400),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "class TrainDatasetFromFolder(Dataset):\n",
    "    def __init__(self, dataset_dir, crop_size, upscale_factor):\n",
    "        super(TrainDatasetFromFolder, self).__init__()\n",
    "        self.image_filenames = [join(dataset_dir, x) for x in listdir(dataset_dir) if is_image_file(x)]\n",
    "        crop_size = calculate_valid_crop_size(crop_size, upscale_factor)\n",
    "        self.hr_transform = train_hr_transform(crop_size)\n",
    "        self.lr_transform = train_lr_transform(crop_size, upscale_factor)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        hr_image = self.hr_transform(Image.open(self.image_filenames[index]))\n",
    "        lr_image = self.lr_transform(hr_image)\n",
    "        return lr_image, hr_image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "class ValDatasetFromFolder(Dataset):\n",
    "    def __init__(self, dataset_dir, upscale_factor):\n",
    "        super(ValDatasetFromFolder, self).__init__()\n",
    "        self.upscale_factor = upscale_factor\n",
    "        self.image_filenames = [join(dataset_dir, x) for x in listdir(dataset_dir) if is_image_file(x)]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        hr_image = Image.open(self.image_filenames[index])\n",
    "        w, h = hr_image.size\n",
    "        crop_size = calculate_valid_crop_size(min(w,h),self.upscale_factor)\n",
    "        lr_scale = transforms.Resize(crop_size // self.upscale_factor, interpolation = Image.BICUBIC)\n",
    "        hr_scale = transforms.Resize(crop_size, interpolation = Image.BICUBIC)\n",
    "        hr_image = transforms.CenterCrop(crop_size)(hr_image)\n",
    "        lr_image = lr_scale(hr_image)\n",
    "        hr_restore_img = hr_scale(lr_image)\n",
    "        return transforms.ToTensor()(lr_image), transforms.ToTensor()(hr_restore_img), transforms.ToTensor()(hr_image)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "class TestDatasetFromFolder(Dataset):\n",
    "    def __init__(self, dataset_dir, upscale_factor):\n",
    "        super(TestDatasetFromFolder, self).__init__()\n",
    "        self.lr_path = dataset_dir + '/SRF_' + str(upscale_factor) + '/data/'\n",
    "        self.hr_path = dataset_dir + '/SRF_' + str(upscale_factor) + '/target/'\n",
    "        self.upscale_factor = upscale_factor\n",
    "        self.lr_filenames = [join(self.lr_path, x) for x in listdir(self.lr_path) if is_image_file(x)]\n",
    "        self.hr_filenames = [join(self.hr_path, x) for x in listdir(self.hr_path) if is_image_file(x)]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.lr_filenames[index].split('/')[-1]\n",
    "        lr_image = Image.open(self.lr_filenames[index])\n",
    "        w,h = lr_image.size\n",
    "        hr_image = Image.open(self.hr_filenames[index])\n",
    "        hr_scale = transforms.Resize((self.upscale_factor * h, self.upscale_factor * w), interpolation=Image.BICUBIC)\n",
    "        hr_restore_img = hr_scale(lr_image)\n",
    "        return image_name, transforms.ToTensor()(lr_image), transforms.ToTensor()(hr_restore_img), transforms.ToTensor()(hr_image)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GeneratorLoss(\n  (loss_network): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (17): ReLU(inplace=True)\n    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (24): ReLU(inplace=True)\n    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (26): ReLU(inplace=True)\n    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n  (mse_loss): MSELoss()\n  (tv_loss): TVLoss()\n)\n"
     ]
    }
   ],
   "source": [
    "# LOSS\n",
    "\n",
    "from torchvision.models.vgg import vgg19\n",
    "\n",
    "\n",
    "class GeneratorLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GeneratorLoss, self).__init__()\n",
    "        vgg = vgg19(pretrained = True)\n",
    "        loss_network = nn.Sequential(*list(vgg.features)[:31]).eval()\n",
    "        for param in loss_network.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.loss_network = loss_network\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.tv_loss = TVLoss()\n",
    "        \n",
    "    def forward(self, out_labels, out_images, target_images):\n",
    "        adversarial_loss = torch.mean(1 - out_labels)\n",
    "        perception_loss = self.mse_loss(self.loss_network(out_images), self.loss_network(target_images))\n",
    "        image_loss = self.mse_loss(out_images, target_images)\n",
    "        tv_loss = self.tv_loss(out_images)\n",
    "        return image_loss + 0.001 * adversarial_loss + 0.006 * perception_loss + 2e-8 * tv_loss\n",
    "    \n",
    "class TVLoss(nn.Module):\n",
    "    def __init__(self, tv_loss_weight=1):\n",
    "        super(TVLoss, self).__init__()\n",
    "        self.tv_loss_weight = tv_loss_weight\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size()[0]\n",
    "        h_x = x.size()[2]\n",
    "        w_x = x.size()[3]\n",
    "        count_h = self.tensor_size(x[:,:,1:,:])\n",
    "        count_w = self.tensor_size(x[:,:,:,1:])\n",
    "        h_tv = torch.pow((x[:,:,1:,:] - x[:,:,:h_x - 1,:]),2).sum()\n",
    "        w_tv = torch.pow((x[:,:,:,1:] - x[:,:,:,:w_x - 1]),2).sum()\n",
    "        return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n",
    "    \n",
    "    @staticmethod\n",
    "    def tensor_size(t):\n",
    "        return t.size()[1] * t.size()[2] * t.size()[3]\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    g_loss = GeneratorLoss()\n",
    "    print(g_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESIDUAL BLOCK\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels,channels, kernel_size=3,padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.prelu = nn.PReLU()\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3,padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        residual = self.conv1(x)\n",
    "        residual = self.bn1(residual)\n",
    "        residual = self.prelu(residual)\n",
    "        residual = self.conv2(residual)\n",
    "        residual = self.bn2(residual)\n",
    "        \n",
    "        return x + residual\n",
    "    \n",
    "    \n",
    "    \n",
    "# UPSAMPLE BLOCK\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    def __init__(self,in_channels, up_scale):\n",
    "        super(UpsampleBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels,in_channels * up_scale ** 2, kernel_size=3, padding=1)\n",
    "        self.pixel_shuffle = nn.PixelShuffle(up_scale)\n",
    "        self.prelu = nn.PReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pixel_shuffle(x)\n",
    "        self.prelu(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "# GENERATOR\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, scale_factor):\n",
    "        upsample_block_num = int(math.log(scale_factor,2))\n",
    "        \n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.block1 = nn.Sequential(nn.Conv2d(3,64,kernel_size=9,padding=4),nn.PReLU())\n",
    "        self.block2 = ResidualBlock(64)\n",
    "        self.block3 = ResidualBlock(64)\n",
    "        self.block4 = ResidualBlock(64)\n",
    "        self.block5 = ResidualBlock(64)\n",
    "        self.block6 = ResidualBlock(64)\n",
    "        self.block7 = nn.Sequential(nn.Conv2d(64,64,kernel_size=3,padding=1), nn.BatchNorm2d(64))\n",
    "        block8 = [UpsampleBlock(64, 2) for _ in range(upsample_block_num)]\n",
    "        block8.append(nn.Conv2d(64,3,kernel_size=9,padding=4))\n",
    "        self.block8 = nn.Sequential(*block8)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        block1 = self.block1(x)\n",
    "        block2 = self.block2(block1)\n",
    "        block3 = self.block3(block2)\n",
    "        block4 = self.block4(block3)\n",
    "        block5 = self.block5(block4)\n",
    "        block6 = self.block6(block5)\n",
    "        block7 = self.block7(block6)\n",
    "        block8 = self.block8(block1 + block7)\n",
    "        \n",
    "        return (torch.tanh(block8) + 1) / 2\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# DISCRIMINATOR\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.net = nn.Sequential(nn.Conv2d(3,64,kernel_size=3,padding=1),\n",
    "                                 nn.LeakyReLU(0.2),\n",
    "                                 \n",
    "                                nn.Conv2d(64,64,kernel_size=3,stride=2,padding=1),\n",
    "                                nn.BatchNorm2d(64), \n",
    "                                nn.LeakyReLU(0.2),\n",
    "                                \n",
    "                                nn.Conv2d(64,128,kernel_size=3,padding=1),\n",
    "                                nn.BatchNorm2d(128), \n",
    "                                nn.LeakyReLU(0.2),\n",
    "                                 \n",
    "                                nn.Conv2d(128,128,kernel_size=3,stride=2,padding=1),\n",
    "                                nn.BatchNorm2d(128), \n",
    "                                nn.LeakyReLU(0.2),\n",
    "                                 \n",
    "                                nn.Conv2d(128,256,kernel_size=3,padding=1),\n",
    "                                nn.BatchNorm2d(256), \n",
    "                                nn.LeakyReLU(0.2),\n",
    "                                 \n",
    "                                nn.Conv2d(256,256,kernel_size=3,stride=2,padding=1),\n",
    "                                nn.BatchNorm2d(256), \n",
    "                                nn.LeakyReLU(0.2),\n",
    "                                 \n",
    "                                nn.Conv2d(256,512,kernel_size=3,padding=1),\n",
    "                                nn.BatchNorm2d(512), \n",
    "                                nn.LeakyReLU(0.2),\n",
    "                                 \n",
    "                                nn.Conv2d(512,512,kernel_size=3,stride=2,padding=1),\n",
    "                                nn.BatchNorm2d(512), \n",
    "                                nn.LeakyReLU(0.2),\n",
    "                                 \n",
    "                                nn.AdaptiveAvgPool2d(1),\n",
    "                                nn.Conv2d(512,1024,kernel_size=1),\n",
    "                                nn.LeakyReLU(0.2),\n",
    "                                nn.Conv2d(1024,1,kernel_size=1)\n",
    "                                )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        return torch.sigmoid(self.net(x).view(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# generator parameters: 586506\n",
      "# discriminator parameters: 5215425\n"
     ]
    }
   ],
   "source": [
    "# netG = Generator(2)\n",
    "# print('# generator parameters:', sum(param.numel() for param in netG.parameters()))\n",
    "# netD = Discriminator()\n",
    "# print('# discriminator parameters:', sum(param.numel() for param in netD.parameters()))\n",
    "\n",
    "# generator_criterion = GeneratorLoss()\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     netG.cuda()\n",
    "#     netD.cuda()\n",
    "#     generator_criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GeneratorLoss(\n",
       "  (loss_network): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (mse_loss): MSELoss()\n",
       "  (tv_loss): TVLoss()\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "netG = Generator(2).eval()\n",
    "netG.cuda()\n",
    "netG.load_state_dict(torch.load('modele/' + 'netG_epoch.pth'))\n",
    "\n",
    "netD = Discriminator().eval()\n",
    "netD.cuda()\n",
    "netD.load_state_dict(torch.load('modele/' + 'netD_epoch.pth'))\n",
    "\n",
    "generator_criterion = GeneratorLoss()\n",
    "generator_criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[1/1000] Loss_D: 1.0000 Loss_G: 0.0069 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:55<00:00,  1.60it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]8.935074762503307\n",
      "   Loss_D   Loss_G       Score_D       Score_G\n",
      "1     1.0  0.00689  1.782052e-40  1.529065e-40\n",
      "[2/1000] Loss_D: 1.0000 Loss_G: 0.0077 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:39<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]17.599195579687755\n",
      "   Loss_D   Loss_G       Score_D       Score_G\n",
      "2     1.0  0.00766  2.039022e-34  1.212483e-35\n",
      "[3/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:39<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]26.261258280277254\n",
      "   Loss_D    Loss_G       Score_D       Score_G\n",
      "3     1.0  0.006768  9.814358e-39  2.887645e-38\n",
      "[4/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:37<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]34.89535812139511\n",
      "   Loss_D    Loss_G       Score_D       Score_G\n",
      "4     1.0  0.006819  2.329411e-37  1.134838e-37\n",
      "[5/1000] Loss_D: 1.0000 Loss_G: 0.0069 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:38<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]43.533286269505815\n",
      "   Loss_D   Loss_G  Score_D  Score_G\n",
      "5     1.0  0.00685      0.0      0.0\n",
      "[6/1000] Loss_D: 1.0000 Loss_G: 0.0069 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:37<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]52.16937504212061\n",
      "   Loss_D    Loss_G       Score_D       Score_G\n",
      "6     1.0  0.006858  4.625695e-38  2.762188e-40\n",
      "[7/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:38<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]60.812668800354004\n",
      "   Loss_D    Loss_G       Score_D       Score_G\n",
      "7     1.0  0.006827  1.157457e-38  1.476431e-38\n",
      "[8/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:38<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]69.46143543322881\n",
      "   Loss_D    Loss_G       Score_D       Score_G\n",
      "8     1.0  0.006839  1.428448e-41  2.177711e-41\n",
      "[9/1000] Loss_D: 1.0000 Loss_G: 0.0075 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:38<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]78.11116205453872\n",
      "   Loss_D    Loss_G       Score_D       Score_G\n",
      "9     1.0  0.007503  4.147206e-35  2.559478e-34\n",
      "[10/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:38<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]86.75352808237076\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "10     1.0  0.006836  3.377437e-38  2.086130e-38\n",
      "[11/1000] Loss_D: 1.0000 Loss_G: 0.0072 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:38<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]95.3986365000407\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "11     1.0  0.007165  1.151554e-39  1.541023e-40\n",
      "[12/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:38<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]104.04546983639399\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "12     1.0  0.006824  1.653295e-34  1.710488e-38\n",
      "[13/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:38<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]112.69518649975458\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "13     1.0  0.006821  3.545549e-34  1.118550e-35\n",
      "[14/1000] Loss_D: 1.0000 Loss_G: 0.0069 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:38<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]121.34351420402527\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "14     1.0  0.006857  5.846626e-36  7.774618e-36\n",
      "[15/1000] Loss_D: 1.0000 Loss_G: 0.0074 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:39<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]129.99932990868885\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "15     1.0  0.007378  2.746075e-40  1.173040e-38\n",
      "[16/1000] Loss_D: 1.0000 Loss_G: 0.0069 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:39<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]138.6598333477974\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "16     1.0  0.006884  1.049031e-36  7.284685e-37\n",
      "[17/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:39<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]147.31564614375432\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "17     1.0  0.006836  5.667652e-42  4.359010e-42\n",
      "[18/1000] Loss_D: 1.0000 Loss_G: 0.0069 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:39<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]155.97336806058883\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "18     1.0  0.006869  4.202766e-43  7.832045e-40\n",
      "[19/1000] Loss_D: 1.0000 Loss_G: 0.0069 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:38<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]164.6254402200381\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "19     1.0  0.006902  3.008817e-36  3.711437e-35\n",
      "[20/1000] Loss_D: 1.0000 Loss_G: 0.0090 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:39<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]173.28182229201\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "20     1.0  0.008953  2.174808e-31  1.321811e-33\n",
      "[21/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:39<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]181.93667855262757\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "21     1.0  0.006843  2.415660e-38  3.806464e-40\n",
      "[22/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:39<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]190.5932519197464\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "22     1.0  0.006758  3.274566e-30  3.353088e-31\n",
      "[23/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:39<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]199.2536915342013\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "23     1.0  0.006808  1.929077e-37  2.077276e-39\n",
      "[24/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:39<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]207.9136874159177\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "24     1.0  0.006826  1.463161e-39  4.142641e-42\n",
      "[25/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:39<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]216.5726707816124\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "25     1.0  0.006834  4.853359e-38  4.712299e-38\n",
      "[26/1000] Loss_D: 1.0000 Loss_G: 0.0069 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:39<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]225.23134827216467\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "26     1.0  0.006859  1.179660e-40  1.338679e-41\n",
      "[27/1000] Loss_D: 1.0000 Loss_G: 0.0072 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:39<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]233.893213613828\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "27     1.0  0.007189  1.844691e-36  4.865230e-35\n",
      "[28/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:39<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]242.5582980553309\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "28     1.0  0.006756  7.256996e-33  1.026848e-33\n",
      "[29/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:39<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]251.22473440170288\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "29     1.0  0.006809  5.897483e-35  1.174913e-36\n",
      "[30/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:39<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]259.89267967542014\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "30     1.0  0.006841  1.905868e-41  8.190485e-41\n",
      "[31/1000] Loss_D: 1.0000 Loss_G: 0.0069 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:39<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]268.56229587395984\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "31     1.0  0.006859  2.204420e-40  2.504654e-39\n",
      "[32/1000] Loss_D: 1.0000 Loss_G: 0.0069 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:40<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]277.2331126968066\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "32     1.0  0.006865  2.597023e-40  2.452708e-40\n",
      "[33/1000] Loss_D: 1.0000 Loss_G: 0.0082 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:39<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]285.9009116013845\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "33     1.0  0.008166  1.087252e-40  3.883496e-39\n",
      "[34/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:40<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]294.5712608218193\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "34     1.0  0.006808  8.574338e-37  1.369576e-37\n",
      "[35/1000] Loss_D: 1.0000 Loss_G: 0.0067 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:39<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]303.24072745641075\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "35     1.0  0.006733  7.516934e-38  1.204461e-36\n",
      "[36/1000] Loss_D: 1.0000 Loss_G: 0.0072 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:41<00:00,  1.64it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]311.93286123275755\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "36     1.0  0.007201  2.304566e-36  1.657232e-36\n",
      "[37/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:40<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]320.60666121641793\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "37     1.0  0.006804  6.261073e-38  3.660794e-38\n",
      "[38/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:39<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]329.26554457743964\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "38     1.0  0.006753  4.013251e-39  5.371349e-40\n",
      "[39/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:38<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]337.91054918368656\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "39     1.0  0.006812  2.121726e-39  7.040555e-40\n",
      "[40/1000] Loss_D: 1.0000 Loss_G: 0.0074 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:38<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]346.549685104688\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "40     1.0  0.007414  5.627162e-40  7.014131e-42\n",
      "[41/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:37<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]355.18616302013396\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "41     1.0  0.006799  3.126446e-36  1.331681e-35\n",
      "[42/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:38<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]363.82694652875267\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "42     1.0  0.006779  3.352354e-38  1.487213e-38\n",
      "[43/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:38<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]372.4668017188708\n",
      "    Loss_D   Loss_G       Score_D       Score_G\n",
      "43     1.0  0.00684  1.361669e-42  2.137911e-42\n",
      "[44/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:38<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]381.10471914609275\n",
      "    Loss_D    Loss_G  Score_D  Score_G\n",
      "44     1.0  0.006832      0.0      0.0\n",
      "[45/1000] Loss_D: 1.0000 Loss_G: 0.0071 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:38<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]389.7437934954961\n",
      "    Loss_D    Loss_G       Score_D  Score_G\n",
      "45     1.0  0.007114  2.540771e-42      0.0\n",
      "[46/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:38<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]398.3898871541023\n",
      "    Loss_D   Loss_G       Score_D       Score_G\n",
      "46     1.0  0.00677  3.642626e-35  3.479683e-34\n",
      "[47/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:38<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]407.0354541023572\n",
      "    Loss_D   Loss_G       Score_D       Score_G\n",
      "47     1.0  0.00679  1.101780e-36  2.187232e-37\n",
      "[48/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:38<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]415.6800718267759\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "48     1.0  0.006835  2.805665e-37  2.933244e-40\n",
      "[49/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:38<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]424.3295054078102\n",
      "    Loss_D   Loss_G       Score_D       Score_G\n",
      "49     1.0  0.00678  1.681956e-39  1.470534e-39\n",
      "[50/1000] Loss_D: 1.0000 Loss_G: 0.0071 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:38<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]432.9767720341682\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "50     1.0  0.007082  2.309788e-39  1.029832e-37\n",
      "[51/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:38<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]441.6265071749687\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "51     1.0  0.006832  1.026522e-34  2.172389e-35\n",
      "[52/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:38<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]450.27750639518104\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "52     1.0  0.006798  3.233324e-42  3.277760e-40\n",
      "[53/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:39<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]458.93565389712654\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "53     1.0  0.006844  1.960615e-39  1.098074e-40\n",
      "[54/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:39<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]467.59003899097445\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "54     1.0  0.006829  2.100908e-38  8.168803e-40\n",
      "[55/1000] Loss_D: 1.0000 Loss_G: 0.0073 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:39<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]476.24391579230627\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "55     1.0  0.007344  4.909096e-38  1.240848e-38\n",
      "[56/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:39<00:00,  1.65it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]484.90071302254995\n",
      "    Loss_D   Loss_G       Score_D       Score_G\n",
      "56     1.0  0.00677  1.307757e-42  3.993383e-42\n",
      "[57/1000] Loss_D: 1.0000 Loss_G: 0.0068 D(x): 0.0000 D(G(z)): 0.0000: 100%|██████████| 857/857 [08:45<00:00,  1.63it/s]\n",
      "  0%|          | 0/857 [00:00<?, ?it/s]493.6577165404955\n",
      "    Loss_D    Loss_G       Score_D       Score_G\n",
      "57     1.0  0.006756  1.370170e-38  4.623307e-37\n",
      "[58/1000] Loss_D: 1.0000 Loss_G: 0.0060 D(x): 0.0000 D(G(z)): 0.0000:   0%|          | 3/857 [00:02<10:31,  1.35it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-3acbde030ed2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mfake_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0md_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mreal_out\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfake_out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0md_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\pytorchenv\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\pytorchenv\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from math import log10\n",
    "import argparse\n",
    "\n",
    "import torch.utils.data\n",
    "import torchvision.utils as utils\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader \n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "\n",
    "\n",
    "from pytorch_msssim import ssim, ms_ssim, SSIM, MS_SSIM\n",
    "import time\n",
    "\n",
    "s = time.time()\n",
    "\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "if __name__ == \"__main__\":\n",
    "    CROP_SIZE = 88\n",
    "    UPSCALE_FACTOR = 2\n",
    "    NUM_EPOCHS = 1000\n",
    "    out_path = 'training_results/SRF_' + str(UPSCALE_FACTOR) + '/'\n",
    "    \n",
    "    train_set = TrainDatasetFromFolder('F:\\CelebAMask-HQ\\VOCdevkit\\VOC2012\\JPEGImages', crop_size = CROP_SIZE, upscale_factor=UPSCALE_FACTOR)\n",
    "    val_set = ValDatasetFromFolder('F:/CelebAMask-HQ/cacatest', upscale_factor=UPSCALE_FACTOR)\n",
    "    train_loader = DataLoader(dataset=train_set, num_workers=0,batch_size=20,shuffle=True)\n",
    "    val_loader = DataLoader(dataset=val_set, num_workers=0, batch_size=1, shuffle=False)\n",
    "    \n",
    "    \n",
    "    optimizerG = torch.optim.Adam(netG.parameters())\n",
    "    optimizerD = torch.optim.Adam(netD.parameters())\n",
    "    \n",
    "    results = {'d_loss': [], 'g_loss': [], 'd_score': [], 'g_score': [], 'psnr': [], 'ssim': []}\n",
    "    \n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        train_bar = tqdm(train_loader)\n",
    "        running_results = {'batch_sizes': 0, 'd_loss': 0, 'g_loss': 0, 'd_score': 0, 'g_score': 0}\n",
    "        \n",
    "        netG.train()\n",
    "        netD.train()\n",
    "        for data, target in train_bar:\n",
    "            g_update_first = True\n",
    "            batch_size = data.size(0)\n",
    "            running_results['batch_sizes'] += batch_size\n",
    "            \n",
    "            real_img = Variable(target)\n",
    "            if torch.cuda.is_available():\n",
    "                real_img = real_img.cuda()\n",
    "            z = Variable(data)\n",
    "            if torch.cuda.is_available():\n",
    "                z = z.cuda()\n",
    "            fake_img = netG(z)\n",
    "            \n",
    "            #UPDATE D network\n",
    "            netD.zero_grad()\n",
    "            real_out = netD(real_img).mean()\n",
    "            fake_out = netD(fake_img).mean()\n",
    "            d_loss = 1 - real_out + fake_out\n",
    "            d_loss.backward(retain_graph=True)\n",
    "            \n",
    "            \n",
    "            #UPDATE G network\n",
    "            netG.zero_grad()\n",
    "            g_loss = generator_criterion(fake_out, fake_img, real_img)\n",
    "            g_loss.backward()\n",
    "            \n",
    "            fake_img = netG(z)\n",
    "            fake_out = netD(fake_img).mean()\n",
    "            \n",
    "            optimizerG.step()\n",
    "            \n",
    "            \n",
    "            optimizerD.step()\n",
    "            \n",
    "            #loss for current batch before oprimization\n",
    "            running_results['g_loss'] += g_loss.item() * batch_size\n",
    "            running_results['d_loss'] += d_loss.item() * batch_size\n",
    "            running_results['d_score'] += real_out.item() * batch_size\n",
    "            running_results['g_score'] += fake_out.item() * batch_size\n",
    "            \n",
    "            train_bar.set_description(desc='[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f' % (\n",
    "                epoch, NUM_EPOCHS, running_results['d_loss'] / running_results['batch_sizes'],\n",
    "                running_results['g_loss'] / running_results['batch_sizes'],\n",
    "                running_results['d_score'] / running_results['batch_sizes'],\n",
    "                running_results['g_score'] / running_results['batch_sizes']))\n",
    "#             print(f'g loss: {running_results[\"g_loss\"]} g score: {running_results[\"g_score\"]}  d loss: {running_results[\"d_loss\"]}  d score: {running_results[\"d_score\"]}')\n",
    "    \n",
    "        netG.eval()\n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image = Image.open('testam.jpg')\n",
    "            image = Variable(ToTensor()(image)).unsqueeze(0)\n",
    "            image = image.cuda()\n",
    "            out = netG(image)\n",
    "            \n",
    "            out_img = ToPILImage()(out[0].data.cpu())\n",
    "            out_img.save('test/testam.jpg')\n",
    "            \n",
    "            print((time.time()-s)/60)\n",
    "#             val_bar = tqdm(val_loader)\n",
    "#             valing_results = {'mse': 0, 'ssims': 0, 'psnr': 0, 'ssim': 0, 'batch_sizes': 0}\n",
    "# #             val_images = []\n",
    "#             for val_lr, val_hr_restore, val_hr in val_bar:\n",
    "#                 batch_size = val_lr.size(0)\n",
    "#                 valing_results['batch_sizes'] += batch_size\n",
    "#                 lr = val_lr\n",
    "#                 hr = val_hr\n",
    "#                 if torch.cuda.is_available():\n",
    "#                     lr = lr.cuda()\n",
    "#                     hr = hr.cuda()\n",
    "#                 sr = netG(lr)\n",
    "\n",
    "#                 batch_mse = ((sr - hr) ** 2).data.mean()\n",
    "#                 valing_results['mse'] += batch_mse * batch_size\n",
    "# #                 batch_ssim = ssim(sr, hr).item()\n",
    "# #                 valing_results['ssims'] += batch_ssim * batch_size\n",
    "#                 valing_results['psnr'] = 10 * log10((hr.max()**2) / (valing_results['mse'] / valing_results['batch_sizes']))\n",
    "#                 valing_results['ssim'] = valing_results['ssims'] / valing_results['batch_sizes']\n",
    "#                 val_bar.set_description(\n",
    "#                     desc='[converting LR images to SR images] PSNR: %.4f dB SSIM: %.4f' % (\n",
    "#                         valing_results['psnr'], valing_results['ssim']))\n",
    "# #                 val_images.extend(\n",
    "# #                     [display_transform()(val_hr_restore.squeeze(0)), display_transform()(hr.data.cpu().squeeze(0)),\n",
    "# #                      display_transform()(sr.data.cpu().squeeze(0))])\n",
    "# #             val_images = torch.stack(val_images)\n",
    "# #             val_images = torch.chunk(val_images,val_images.size(0) // 1)\n",
    "# #             val_save_bar = tqdm(val_images, desc='savin training results')\n",
    "# #             index = 1\n",
    "# #             for image in val_images:\n",
    "# #                 image = utils.make_grid(image, nrow=3,padding=5)\n",
    "# #                 utils.save_image(image, out_path + 'epoch_%d_index_%d.png' % (epoch, index), padding=5)\n",
    "# #                 index += 1\n",
    "                \n",
    "    \n",
    "        torch.save(netG.state_dict(), 'modele/netG_epoch.pth')\n",
    "        torch.save(netD.state_dict(), 'modele/netD_epoch.pth')\n",
    "\n",
    "        results['d_loss'].append(running_results['d_loss'] / running_results['batch_sizes'])\n",
    "        results['g_loss'].append(running_results['g_loss'] / running_results['batch_sizes'])\n",
    "        results['d_score'].append(running_results['d_score'] / running_results['batch_sizes'])\n",
    "        results['g_score'].append(running_results['g_score'] / running_results['batch_sizes'])\n",
    "#         results['psnr'].append(valing_results['psnr'])\n",
    "#         results['ssim'].append(valing_results['ssim'])\n",
    "\n",
    "#         if epoch % 1 == 0 and epoch != 0:\n",
    "#             out_path = 'statistics/'\n",
    "        data_frame = pd.DataFrame(\n",
    "            data={'Loss_D': results['d_loss'], 'Loss_G': results['g_loss'], 'Score_D': results['d_score'],\n",
    "                  'Score_G': results['g_score']},\n",
    "                index=range(1, epoch + 1))\n",
    "#                 data_frame.to_csv(out_path + 'srf_' + str(UPSCALE_FACTOR) + '_train_results.csv', index_label='Epoch')\n",
    "        print(data_frame.iloc[[-1]])\n",
    "\n",
    "print(f'mins:  {(time.time()-s)/60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "\n",
    "UPSCALE_FACTOR = 2\n",
    "TEST_MODE = True\n",
    "IMAGE_NAME = 'pu.jpg'\n",
    "MODEL_NAME = 'netG_epoch.pth'\n",
    "\n",
    "model = Generator(UPSCALE_FACTOR).eval()\n",
    "model.cuda()\n",
    "model.load_state_dict(torch.load('modele/' + MODEL_NAME))\n",
    "\n",
    "image = Image.open(IMAGE_NAME)\n",
    "image = Variable(transforms.ToTensor()(image)).unsqueeze(0)\n",
    "image = image.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(image)\n",
    "    \n",
    "out_img = ToPILImage()(out[0].data.cpu())\n",
    "out_img.save('test/' + str('20') + '_' + IMAGE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1662\n"
     ]
    }
   ],
   "source": [
    "# path = 'F:/CelebAMask-HQ/VOCdevkit/VOC2012/JPEGImages'\n",
    "# img_names = []\n",
    "\n",
    "# for folder,subfolders,filenames in os.walk(path):\n",
    "#     for img in filenames:\n",
    "#         img_names.append(folder+'/'+img)\n",
    "        \n",
    "# len(img_names)\n",
    "\n",
    "# img_sizes = []\n",
    "# rejected = []\n",
    "\n",
    "# for item in img_names:\n",
    "#     try:\n",
    "#         with Image.open(item) as img:\n",
    "#             img_sizes.append(img.size)\n",
    "            \n",
    "#     except:\n",
    "#         rejected.append(item)\n",
    "        \n",
    "# df = pd.DataFrame(img_sizes)\n",
    "# x = 0\n",
    "# for i, row in df.iterrows():\n",
    "#     if row[1] < 88:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "      <td>2000</td>\n",
       "      <td>3000</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      a     b     c     d\n",
       "2  1000  2000  3000  4000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python373jvsc74a57bd04e68a42444b13db3bacd1cd6c46a066f96ab621132701a69454eeb8e1d76a336",
   "display_name": "Python 3.7.3 64-bit ('pytorchenv': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}