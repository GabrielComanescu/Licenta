{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARPLAYER\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "backwarp_tenGrid = {}\n",
    "\n",
    "def warp(tenInput, tenFlow):\n",
    "    k = (str(tenFlow.device), str(tenFlow.size()))\n",
    "    if k not in backwarp_tenGrid:\n",
    "        tenHorizontal = torch.linspace(-1.0, 1.0, tenFlow.shape[3], device=device).view(\n",
    "            1, 1, 1, tenFlow.shape[3]).expand(tenFlow.shape[0], -1, tenFlow.shape[2], -1)\n",
    "        tenVertical = torch.linspace(-1.0, 1.0, tenFlow.shape[2], device=device).view(\n",
    "            1, 1, tenFlow.shape[2], 1).expand(tenFlow.shape[0], -1, -1, tenFlow.shape[3])\n",
    "        backwarp_tenGrid[k] = torch.cat(\n",
    "            [tenHorizontal, tenVertical], 1).to(device)\n",
    "        \n",
    "    tenFlow = torch.cat([tenFlow[:, 0:1, :, :] / ((tenInput.shape[3] - 1.0) / 2.0),\n",
    "                         tenFlow[:, 1:2, :, :] / ((tenInput.shape[2] - 1.0) / 2.0)], 1)\n",
    "    \n",
    "    g = (backwarp_tenGrid[k] + tenFlow).permute(0,2,3,1)\n",
    "    return F.grid_sample(input=tenInput, grid = g, mode = 'bilinear', padding_mode='border', align_corners=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOSS\n",
    "\n",
    "class EPE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EPE, self).__init__()\n",
    "\n",
    "    def forward(self, flow, gt, loss_mask):\n",
    "        loss_map = (flow - gt.detach()) ** 2\n",
    "        loss_map = (loss_map.sum(1, True) + 1e-6) ** 0.5\n",
    "        return (loss_map * loss_mask)\n",
    "\n",
    "\n",
    "class Ternary(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Ternary, self).__init__()\n",
    "        patch_size = 7\n",
    "        out_channels = patch_size * patch_size\n",
    "        self.w = np.eye(out_channels).reshape(\n",
    "            (patch_size, patch_size, 1, out_channels))\n",
    "        self.w = np.transpose(self.w, (3, 2, 0, 1))\n",
    "        self.w = torch.tensor(self.w).float().to(device)\n",
    "\n",
    "    def transform(self, img):\n",
    "        patches = F.conv2d(img, self.w, padding=3, bias=None)\n",
    "        transf = patches - img\n",
    "        transf_norm = transf / torch.sqrt(0.81 + transf**2)\n",
    "        return transf_norm\n",
    "\n",
    "    def rgb2gray(self, rgb):\n",
    "        r, g, b = rgb[:, 0:1, :, :], rgb[:, 1:2, :, :], rgb[:, 2:3, :, :]\n",
    "        gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "        return gray\n",
    "\n",
    "    def hamming(self, t1, t2):\n",
    "        dist = (t1 - t2) ** 2\n",
    "        dist_norm = torch.mean(dist / (0.1 + dist), 1, True)\n",
    "        return dist_norm\n",
    "\n",
    "    def valid_mask(self, t, padding):\n",
    "        n, _, h, w = t.size()\n",
    "        inner = torch.ones(n, 1, h - 2 * padding, w - 2 * padding).type_as(t)\n",
    "        mask = F.pad(inner, [padding] * 4)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, img0, img1):\n",
    "        img0 = self.transform(self.rgb2gray(img0))\n",
    "        img1 = self.transform(self.rgb2gray(img1))\n",
    "        return self.hamming(img0, img1) * self.valid_mask(img0, 1)\n",
    "\n",
    "\n",
    "class SOBEL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SOBEL, self).__init__()\n",
    "        self.kernelX = torch.tensor([\n",
    "            [1, 0, -1],\n",
    "            [2, 0, -2],\n",
    "            [1, 0, -1],\n",
    "        ]).float()\n",
    "        self.kernelY = self.kernelX.clone().T\n",
    "        self.kernelX = self.kernelX.unsqueeze(0).unsqueeze(0).to(device)\n",
    "        self.kernelY = self.kernelY.unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "    def forward(self, pred, gt):\n",
    "        N, C, H, W = pred.shape[0], pred.shape[1], pred.shape[2], pred.shape[3]\n",
    "        img_stack = torch.cat(\n",
    "            [pred.reshape(N*C, 1, H, W), gt.reshape(N*C, 1, H, W)], 0)\n",
    "        sobel_stack_x = F.conv2d(img_stack, self.kernelX, padding=1)\n",
    "        sobel_stack_y = F.conv2d(img_stack, self.kernelY, padding=1)\n",
    "        pred_X, gt_X = sobel_stack_x[:N*C], sobel_stack_x[N*C:]\n",
    "        pred_Y, gt_Y = sobel_stack_y[:N*C], sobel_stack_y[N*C:]\n",
    "\n",
    "        L1X, L1Y = torch.abs(pred_X-gt_X), torch.abs(pred_Y-gt_Y)\n",
    "        loss = (L1X+L1Y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IFNet MODEL\n",
    "\n",
    "def conv_wo_act(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                  padding=padding, dilation=dilation, bias=True),\n",
    "    )\n",
    "\n",
    "def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                  padding=padding, dilation=dilation, bias=True),\n",
    "        nn.PReLU(out_planes)\n",
    "    )\n",
    "\n",
    "#Block pt retea\n",
    "class IFBlock(nn.Module):\n",
    "    def __init__(self, in_planes, scale=1, c=64):\n",
    "        super(IFBlock, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.conv0 = nn.Sequential(\n",
    "            conv(in_planes, c, 3, 2, 1),\n",
    "            conv(c, 2*c, 3, 2, 1),\n",
    "            )\n",
    "        self.convblock = nn.Sequential(\n",
    "            conv(2*c, 2*c),\n",
    "            conv(2*c, 2*c),\n",
    "            conv(2*c, 2*c),\n",
    "            conv(2*c, 2*c),\n",
    "            conv(2*c, 2*c),\n",
    "            conv(2*c, 2*c),\n",
    "        )        \n",
    "        self.conv1 = nn.ConvTranspose2d(2*c, 4, 4, 2, 1)\n",
    "                    \n",
    "    def forward(self, x):\n",
    "        if self.scale != 1:\n",
    "            x = F.interpolate(x, scale_factor=1. / self.scale, mode=\"bilinear\",\n",
    "                              align_corners=False)\n",
    "        x = self.conv0(x)\n",
    "        x = self.convblock(x)\n",
    "        x = self.conv1(x)\n",
    "        flow = x\n",
    "        if self.scale != 1:\n",
    "            flow = F.interpolate(flow, scale_factor=self.scale, mode=\"bilinear\",\n",
    "                                 align_corners=False)\n",
    "        return flow\n",
    "\n",
    "#reteaua ifnet\n",
    "class IFNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IFNet, self).__init__()\n",
    "        self.block0 = IFBlock(6, scale=8, c=192)\n",
    "        self.block1 = IFBlock(10, scale=4, c=128)\n",
    "        self.block2 = IFBlock(10, scale=2, c=96)\n",
    "        self.block3 = IFBlock(10, scale=1, c=48)\n",
    "\n",
    "    def forward(self, x, scale=1.0):\n",
    "        if scale != 1.0:\n",
    "            x = F.interpolate(x, scale_factor=scale, mode=\"bilinear\", align_corners=False)\n",
    "        flow0 = self.block0(x)\n",
    "        F1 = flow0\n",
    "        F1_large = F.interpolate(F1, scale_factor=2.0, mode=\"bilinear\", align_corners=False) * 2.0\n",
    "        warped_img0 = warp(x[:, :3], F1_large[:, :2])\n",
    "        warped_img1 = warp(x[:, 3:], F1_large[:, 2:4])\n",
    "        flow1 = self.block1(torch.cat((warped_img0, warped_img1, F1_large), 1))\n",
    "        F2 = (flow0 + flow1)\n",
    "        F2_large = F.interpolate(F2, scale_factor=2.0, mode=\"bilinear\", align_corners=False) * 2.0\n",
    "        warped_img0 = warp(x[:, :3], F2_large[:, :2])\n",
    "        warped_img1 = warp(x[:, 3:], F2_large[:, 2:4])\n",
    "        flow2 = self.block2(torch.cat((warped_img0, warped_img1, F2_large), 1))\n",
    "        F3 = (flow0 + flow1 + flow2)\n",
    "        F3_large = F.interpolate(F3, scale_factor=2.0, mode=\"bilinear\", align_corners=False) * 2.0\n",
    "        warped_img0 = warp(x[:, :3], F3_large[:, :2])\n",
    "        warped_img1 = warp(x[:, 3:], F3_large[:, 2:4])\n",
    "        flow3 = self.block3(torch.cat((warped_img0, warped_img1, F3_large), 1))\n",
    "        F4 = (flow0 + flow1 + flow2 + flow3)\n",
    "        if scale != 1.0:\n",
    "            F4 = F.interpolate(F4, scale_factor=1 / scale, mode=\"bilinear\", align_corners=False) / scale\n",
    "        return F4, [F1, F2, F3, F4]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RIFE MODEL\n",
    "\n",
    "from torch.optim import AdamW\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                  padding=padding, dilation=dilation, bias=True),\n",
    "        nn.PReLU(out_planes)\n",
    "    )\n",
    "\n",
    "\n",
    "def deconv(in_planes, out_planes, kernel_size=4, stride=2, padding=1):\n",
    "    return nn.Sequential(\n",
    "        torch.nn.ConvTranspose2d(in_channels=in_planes, out_channels=out_planes,\n",
    "                                 kernel_size=4, stride=2, padding=1, bias=True),\n",
    "        nn.PReLU(out_planes)\n",
    "    )\n",
    "\n",
    "def conv_woact(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                  padding=padding, dilation=dilation, bias=True),\n",
    "    )\n",
    "\n",
    "class Conv2(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride=2):\n",
    "        super(Conv2, self).__init__()\n",
    "        self.conv1 = conv(in_planes, out_planes, 3, stride, 1)\n",
    "        self.conv2 = conv(out_planes, out_planes, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "c = 32\n",
    "\n",
    "class ContextNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ContextNet, self).__init__()\n",
    "        self.conv0 = Conv2(3, c)\n",
    "        self.conv1 = Conv2(c, c)\n",
    "        self.conv2 = Conv2(c, 2*c)\n",
    "        self.conv3 = Conv2(2*c, 4*c)\n",
    "        self.conv4 = Conv2(4*c, 8*c)\n",
    "\n",
    "    def forward(self, x, flow):\n",
    "        x = self.conv0(x)\n",
    "        x = self.conv1(x)\n",
    "        flow = F.interpolate(flow, scale_factor=0.5, mode=\"bilinear\", align_corners=False) * 0.5\n",
    "        f1 = warp(x, flow)\n",
    "        x = self.conv2(x)\n",
    "        flow = F.interpolate(flow, scale_factor=0.5, mode=\"bilinear\",\n",
    "                             align_corners=False) * 0.5\n",
    "        f2 = warp(x, flow)\n",
    "        x = self.conv3(x)\n",
    "        flow = F.interpolate(flow, scale_factor=0.5, mode=\"bilinear\",\n",
    "                             align_corners=False) * 0.5\n",
    "        f3 = warp(x, flow)\n",
    "        x = self.conv4(x)\n",
    "        flow = F.interpolate(flow, scale_factor=0.5, mode=\"bilinear\",\n",
    "                             align_corners=False) * 0.5\n",
    "        f4 = warp(x, flow)\n",
    "        return [f1, f2, f3, f4]\n",
    "\n",
    "\n",
    "class FusionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FusionNet, self).__init__()\n",
    "        self.conv0 = Conv2(10, c)\n",
    "        self.down0 = Conv2(c, 2*c)\n",
    "        self.down1 = Conv2(4*c, 4*c)\n",
    "        self.down2 = Conv2(8*c, 8*c)\n",
    "        self.down3 = Conv2(16*c, 16*c)\n",
    "        self.up0 = deconv(32*c, 8*c)\n",
    "        self.up1 = deconv(16*c, 4*c)\n",
    "        self.up2 = deconv(8*c, 2*c)\n",
    "        self.up3 = deconv(4*c, c)\n",
    "        self.conv = nn.ConvTranspose2d(c, 4, 4, 2, 1)\n",
    "\n",
    "    def forward(self, img0, img1, flow, c0, c1, flow_gt):\n",
    "        warped_img0 = warp(img0, flow[:, :2])\n",
    "        warped_img1 = warp(img1, flow[:, 2:4])\n",
    "        if flow_gt == None:\n",
    "            warped_img0_gt, warped_img1_gt = None, None\n",
    "        else:\n",
    "            warped_img0_gt = warp(img0, flow_gt[:, :2])\n",
    "            warped_img1_gt = warp(img1, flow_gt[:, 2:4])\n",
    "        x = self.conv0(torch.cat((warped_img0, warped_img1, flow), 1))\n",
    "        s0 = self.down0(x)\n",
    "        s1 = self.down1(torch.cat((s0, c0[0], c1[0]), 1))\n",
    "        s2 = self.down2(torch.cat((s1, c0[1], c1[1]), 1))\n",
    "        s3 = self.down3(torch.cat((s2, c0[2], c1[2]), 1))\n",
    "        x = self.up0(torch.cat((s3, c0[3], c1[3]), 1))\n",
    "        x = self.up1(torch.cat((x, s2), 1))\n",
    "        x = self.up2(torch.cat((x, s1), 1))\n",
    "        x = self.up3(torch.cat((x, s0), 1))\n",
    "        x = self.conv(x)\n",
    "        return x, warped_img0, warped_img1, warped_img0_gt, warped_img1_gt\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, local_rank=-1):\n",
    "        self.flownet = IFNet()\n",
    "        self.contextnet = ContextNet()\n",
    "        self.fusionnet = FusionNet()\n",
    "        self.device()\n",
    "        self.optimG = AdamW(itertools.chain(\n",
    "            self.flownet.parameters(),\n",
    "            self.contextnet.parameters(),\n",
    "            self.fusionnet.parameters()), lr=1e-6, weight_decay=1e-5)\n",
    "        self.schedulerG = optim.lr_scheduler.CyclicLR(\n",
    "            self.optimG, base_lr=1e-6, max_lr=1e-3, step_size_up=8000, cycle_momentum=False)\n",
    "        self.epe = EPE()\n",
    "        self.ter = Ternary()\n",
    "        self.sobel = SOBEL()\n",
    "        if local_rank != -1:\n",
    "            self.flownet = DDP(self.flownet, device_ids=[\n",
    "                               local_rank], output_device=local_rank)\n",
    "            self.contextnet = DDP(self.contextnet, device_ids=[\n",
    "                                  local_rank], output_device=local_rank)\n",
    "            self.fusionnet = DDP(self.fusionnet, device_ids=[\n",
    "                                 local_rank], output_device=local_rank)\n",
    "\n",
    "    def train(self):\n",
    "        self.flownet.train()\n",
    "        self.contextnet.train()\n",
    "        self.fusionnet.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.flownet.eval()\n",
    "        self.contextnet.eval()\n",
    "        self.fusionnet.eval()\n",
    "\n",
    "    def device(self):\n",
    "        self.flownet.to(device)\n",
    "        self.contextnet.to(device)\n",
    "        self.fusionnet.to(device)\n",
    "\n",
    "    def load_model(self, path, rank):\n",
    "        def convert(param):\n",
    "            if rank == -1:\n",
    "                return {\n",
    "                    k.replace(\"module.\", \"\"): v\n",
    "                    for k, v in param.items()\n",
    "                    if \"module.\" in k\n",
    "                }\n",
    "            else:\n",
    "                return param\n",
    "        if rank <= 0:\n",
    "            self.flownet.load_state_dict(\n",
    "                convert(torch.load('{}/flownet.pkl'.format(path), map_location=device)))\n",
    "            self.contextnet.load_state_dict(\n",
    "                convert(torch.load('{}/contextnet.pkl'.format(path), map_location=device)))\n",
    "            self.fusionnet.load_state_dict(\n",
    "                convert(torch.load('{}/unet.pkl'.format(path), map_location=device)))\n",
    "\n",
    "    def save_model(self, path, rank):\n",
    "        if rank == 0:\n",
    "            torch.save(self.flownet.state_dict(), '{}/flownet.pkl'.format(path))\n",
    "            torch.save(self.contextnet.state_dict(), '{}/contextnet.pkl'.format(path))\n",
    "            torch.save(self.fusionnet.state_dict(), '{}/unet.pkl'.format(path))\n",
    "\n",
    "    def predict(self, imgs, flow, training=True, flow_gt=None):\n",
    "        img0 = imgs[:, :3]\n",
    "        img1 = imgs[:, 3:]\n",
    "        c0 = self.contextnet(img0, flow[:, :2])\n",
    "        c1 = self.contextnet(img1, flow[:, 2:4])\n",
    "        flow = F.interpolate(flow, scale_factor=2.0, mode=\"bilinear\",\n",
    "                             align_corners=False) * 2.0\n",
    "        refine_output, warped_img0, warped_img1, warped_img0_gt, warped_img1_gt = self.fusionnet(\n",
    "            img0, img1, flow, c0, c1, flow_gt)\n",
    "        res = torch.sigmoid(refine_output[:, :3]) * 2 - 1\n",
    "        mask = torch.sigmoid(refine_output[:, 3:4])\n",
    "        merged_img = warped_img0 * mask + warped_img1 * (1 - mask)\n",
    "        pred = merged_img + res\n",
    "        pred = torch.clamp(pred, 0, 1)\n",
    "        if training:\n",
    "            return pred, mask, merged_img, warped_img0, warped_img1, warped_img0_gt, warped_img1_gt\n",
    "        else:\n",
    "            return pred\n",
    "\n",
    "    def inference(self, img0, img1, scale=1.0):\n",
    "        imgs = torch.cat((img0, img1), 1)\n",
    "        flow, _ = self.flownet(imgs, scale)\n",
    "        return self.predict(imgs, flow, training=False)\n",
    "\n",
    "    def update(self, imgs, gt, learning_rate=0, mul=1, training=True, flow_gt=None):\n",
    "        for param_group in self.optimG.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "        if training:\n",
    "            self.train()\n",
    "        else:\n",
    "            self.eval()\n",
    "        flow, flow_list = self.flownet(imgs)\n",
    "        pred, mask, merged_img, warped_img0, warped_img1, warped_img0_gt, warped_img1_gt = self.predict(\n",
    "            imgs, flow, flow_gt=flow_gt)\n",
    "        loss_ter = self.ter(pred, gt).mean()\n",
    "        if training:\n",
    "            with torch.no_grad():\n",
    "                loss_flow = torch.abs(warped_img0_gt - gt).mean()\n",
    "                loss_mask = torch.abs(\n",
    "                    merged_img - gt).sum(1, True).float().detach()\n",
    "                loss_mask = F.interpolate(loss_mask, scale_factor=0.5, mode=\"bilinear\",\n",
    "                                          align_corners=False).detach()\n",
    "                flow_gt = (F.interpolate(flow_gt, scale_factor=0.5, mode=\"bilinear\",\n",
    "                                         align_corners=False) * 0.5).detach()\n",
    "            loss_cons = 0\n",
    "            for i in range(4):\n",
    "                loss_cons += self.epe(flow_list[i][:, :2], flow_gt[:, :2], 1)\n",
    "                loss_cons += self.epe(flow_list[i][:, 2:4], flow_gt[:, 2:4], 1)\n",
    "            loss_cons = loss_cons.mean() * 0.01\n",
    "        else:\n",
    "            loss_cons = torch.tensor([0])\n",
    "            loss_flow = torch.abs(warped_img0 - gt).mean()\n",
    "            loss_mask = 1\n",
    "        loss_l1 = (((pred - gt) ** 2 + 1e-6) ** 0.5).mean()\n",
    "        if training:\n",
    "            self.optimG.zero_grad()\n",
    "            loss_G = loss_l1 + loss_cons + loss_ter\n",
    "            loss_G.backward()\n",
    "            self.optimG.step()\n",
    "        return pred, merged_img, flow, loss_l1, loss_flow, loss_cons, loss_ter, loss_mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASET\n",
    "\n",
    "import cv2\n",
    "import ast\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "cv2.setNumThreads(1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class VimeoDataset(Dataset):\n",
    "    def __init__(self, dataset_name, batch_size=32):\n",
    "        self.batch_size = batch_size\n",
    "        self.path = 'F:\\\\Goby\\\\vimeo_triplet\\\\sequences'\n",
    "        self.dataset_name = dataset_name\n",
    "        self.load_data()\n",
    "        self.h = 256\n",
    "        self.w = 448\n",
    "        xx = np.arange(0, self.w).reshape(1,-1).repeat(self.h,0)\n",
    "        yy = np.arange(0, self.h).reshape(-1,1).repeat(self.w,1)\n",
    "        self.grid = np.stack((xx,yy),2).copy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta_data)\n",
    "\n",
    "    def load_data(self):\n",
    "        self.train_data = []\n",
    "        self.flow_data = []\n",
    "        self.val_data = []\n",
    "        for i in range(100):\n",
    "            f = np.load('dataset/{}.npz'.format(i))\n",
    "            if i < 80:\n",
    "                self.train_data.append(f['i0i1gt'])\n",
    "                self.flow_data.append(f['ft0ft1'])\n",
    "            else:\n",
    "                self.val_data.append(f['i0i1gt'])\n",
    "        if self.dataset_name == 'train':\n",
    "            self.meta_data = self.train_data\n",
    "        else:\n",
    "            self.meta_data = self.val_data\n",
    "        self.nr_sample = len(self.meta_data)        \n",
    "\n",
    "    def aug(self, img0, gt, img1, flow_gt, h, w):\n",
    "        ih, iw, _ = img0.shape\n",
    "        x = np.random.randint(0, ih - h + 1)\n",
    "        y = np.random.randint(0, iw - w + 1)\n",
    "        img0 = img0[x:x+h, y:y+w, :]\n",
    "        img1 = img1[x:x+h, y:y+w, :]\n",
    "        gt = gt[x:x+h, y:y+w, :]\n",
    "        flow_gt = flow_gt[x:x+h, y:y+w, :]\n",
    "        return img0, gt, img1, flow_gt\n",
    "\n",
    "    def getimg(self, index):\n",
    "        data = self.meta_data[index]\n",
    "        img0 = data[0:3].transpose(1, 2, 0)\n",
    "        img1 = data[3:6].transpose(1, 2, 0)\n",
    "        gt = data[6:9].transpose(1, 2, 0)\n",
    "        flow_gt = (self.flow_data[index]).transpose(1, 2, 0)\n",
    "        return img0, gt, img1, flow_gt\n",
    "            \n",
    "    def __getitem__(self, index):        \n",
    "        img0, gt, img1, flow_gt = self.getimg(index)\n",
    "        if self.dataset_name == 'train':\n",
    "            img0, gt, img1, flow_gt = self.aug(img0, gt, img1, flow_gt, 224, 224)\n",
    "            if random.uniform(0, 1) < 0.5:\n",
    "                img0 = img0[:, :, ::-1]\n",
    "                img1 = img1[:, :, ::-1]\n",
    "                gt = gt[:, :, ::-1]\n",
    "            if random.uniform(0, 1) < 0.5:\n",
    "                img0 = img0[::-1]\n",
    "                img1 = img1[::-1]\n",
    "                gt = gt[::-1]\n",
    "                flow_gt = flow_gt[::-1]\n",
    "                flow_gt = np.concatenate((flow_gt[:, :, 0:1], -flow_gt[:, :, 1:2], flow_gt[:, :, 2:3], -flow_gt[:, :, 3:4]), 2)\n",
    "            if random.uniform(0, 1) < 0.5:\n",
    "                img0 = img0[:, ::-1]\n",
    "                img1 = img1[:, ::-1]\n",
    "                gt = gt[:, ::-1]\n",
    "                flow_gt = flow_gt[:, ::-1]\n",
    "                flow_gt = np.concatenate((-flow_gt[:, :, 0:1], flow_gt[:, :, 1:2], -flow_gt[:, :, 2:3], flow_gt[:, :, 3:4]), 2)\n",
    "            if random.uniform(0, 1) < 0.5:\n",
    "                tmp = img1\n",
    "                img1 = img0\n",
    "                img0 = tmp\n",
    "                flow_gt = np.concatenate((flow_gt[:, :, 2:4], flow_gt[:, :, 0:2]), 2)\n",
    "        else:\n",
    "            flow_gt = np.zeros((256, 448, 4))\n",
    "        flow_gt = torch.from_numpy(flow_gt.copy()).permute(2, 0, 1)\n",
    "        img0 = torch.from_numpy(img0.copy()).permute(2, 0, 1)\n",
    "        img1 = torch.from_numpy(img1.copy()).permute(2, 0, 1)\n",
    "        gt = torch.from_numpy(gt.copy()).permute(2, 0, 1)\n",
    "        return torch.cat((img0, img1, gt), 0), flow_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset/0.npz'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-6095d18bdbc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbenchmark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-6095d18bdbc0>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mnr_eval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVimeoDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0msampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDistributedSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msampler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-cb477c59e6fb>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset_name, batch_size)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'F:\\\\Goby\\\\vimeo_triplet\\\\sequences'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m448\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-cb477c59e6fb>\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dataset/{}.npz'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'i0i1gt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\pytorchenv\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/0.npz'"
     ]
    }
   ],
   "source": [
    "#TRAIN\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "import torch.distributed as dist\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "def get_learning_rate(step):\n",
    "    if step < 2000:\n",
    "        mul = step / 2000.\n",
    "    else:\n",
    "        mul = np.cos((step - 2000) / (args.epoch * args.step_per_epoch - 2000.) * math.pi) * 0.5 + 0.5\n",
    "    return 1e-4 * mul\n",
    "\n",
    "def flow2rgb(flow_map_np):\n",
    "    h, w, _ = flow_map_np.shape\n",
    "    rgb_map = np.ones((h, w, 3)).astype(np.float32)\n",
    "    normalized_flow_map = flow_map_np / (np.abs(flow_map_np).max())\n",
    "    \n",
    "    rgb_map[:, :, 0] += normalized_flow_map[:, :, 0]\n",
    "    rgb_map[:, :, 1] -= 0.5 * (normalized_flow_map[:, :, 0] + normalized_flow_map[:, :, 1])\n",
    "    rgb_map[:, :, 2] += normalized_flow_map[:, :, 1]\n",
    "    return rgb_map.clip(0, 1)\n",
    "\n",
    "def train(model):\n",
    "    log_path = 'train_log'\n",
    "    if local_rank == 0:\n",
    "        writer = SummaryWriter(log_path + '/train')\n",
    "        writer_val = SummaryWriter(log_path + '/validate')\n",
    "    else:\n",
    "        writer, writer_val = None, None\n",
    "    step = 0\n",
    "    nr_eval = 0\n",
    "    dataset = VimeoDataset('train')\n",
    "    sampler = DistributedSampler(dataset)\n",
    "    train_data = DataLoader(dataset, batch_size=args.batch_size, num_workers=8, pin_memory=True, drop_last=True, sampler=sampler)\n",
    "    args.step_per_epoch = train_data.__len__()\n",
    "    dataset_val = VimeoDataset('validation')\n",
    "    val_data = DataLoader(dataset_val, batch_size=12, pin_memory=True, num_workers=8)\n",
    "    evaluate(model, val_data, nr_eval, local_rank, writer_val)\n",
    "    model.save_model(log_path, local_rank)\n",
    "    print('training...')\n",
    "    time_stamp = time.time()\n",
    "    for epoch in range(args.epoch):\n",
    "        sampler.set_epoch(epoch)\n",
    "        for i, data in enumerate(train_data):\n",
    "            data_time_interval = time.time() - time_stamp\n",
    "            time_stamp = time.time()\n",
    "            data_gpu, flow_gt = data\n",
    "            data_gpu = data_gpu.to(device, non_blocking=True) / 255.\n",
    "            flow_gt = flow_gt.to(device, non_blocking=True)\n",
    "            imgs = data_gpu[:, :6]\n",
    "            gt = data_gpu[:, 6:9]\n",
    "            mul = np.cos(step / (args.epoch * args.step_per_epoch) * math.pi) * 0.5 + 0.5\n",
    "            learning_rate = get_learning_rate(step)\n",
    "            pred, merged_img, flow, loss_l1, loss_flow, loss_cons, loss_ter, flow_mask = model.update(imgs, gt, learning_rate, mul, True, flow_gt)\n",
    "            train_time_interval = time.time() - time_stamp\n",
    "            time_stamp = time.time()\n",
    "            if step % 100 == 1 and local_rank == 0:\n",
    "                writer.add_scalar('learning_rate', learning_rate, step)\n",
    "                writer.add_scalar('loss_l1', loss_l1, step)\n",
    "                writer.add_scalar('loss_flow', loss_flow, step)\n",
    "                writer.add_scalar('loss_cons', loss_cons, step)\n",
    "                writer.add_scalar('loss_ter', loss_ter, step)\n",
    "            if step % 1000 == 1 and local_rank == 0:\n",
    "                gt = (gt.permute(0, 2, 3, 1).detach().cpu().numpy() * 255).astype('uint8')\n",
    "                pred = (pred.permute(0, 2, 3, 1).detach().cpu().numpy() * 255).astype('uint8')\n",
    "                merged_img = (merged_img.permute(0, 2, 3, 1).detach().cpu().numpy() * 255).astype('uint8')\n",
    "                flow = flow.permute(0, 2, 3, 1).detach().cpu().numpy()\n",
    "                flow_mask = flow_mask.permute(0, 2, 3, 1).detach().cpu().numpy()\n",
    "                flow_gt = flow_gt.permute(0, 2, 3, 1).detach().cpu().numpy()\n",
    "                for i in range(5):\n",
    "                    imgs = np.concatenate((merged_img[i], pred[i], gt[i]), 1)[:, :, ::-1]\n",
    "                    writer.add_image(str(i) + '/img', imgs, step, dataformats='HWC')\n",
    "                    writer.add_image(str(i) + '/flow', flow2rgb(flow[i]), step, dataformats='HWC')\n",
    "                    writer.add_image(str(i) + '/flow_gt', flow2rgb(flow_gt[i]), step, dataformats='HWC')\n",
    "                    writer.add_image(str(i) + '/flow_mask', flow2rgb(flow[i] * flow_mask[i]), step, dataformats='HWC')\n",
    "                writer.flush()\n",
    "            if local_rank == 0:\n",
    "                print('epoch:{} {}/{} time:{:.2f}+{:.2f} loss_l1:{:.4e}'.format(epoch, i, args.step_per_epoch, data_time_interval, train_time_interval, loss_l1))\n",
    "            step += 1\n",
    "        nr_eval += 1\n",
    "        if nr_eval % 5 == 0:\n",
    "            evaluate(model, val_data, step, local_rank, writer_val)\n",
    "        model.save_model(log_path, local_rank)    \n",
    "        dist.barrier()\n",
    "\n",
    "def evaluate(model, val_data, nr_eval, local_rank, writer_val):\n",
    "    loss_l1_list = []\n",
    "    loss_cons_list = []\n",
    "    loss_ter_list = []\n",
    "    loss_flow_list = []\n",
    "    psnr_list = []\n",
    "    time_stamp = time.time()\n",
    "    for i, data in enumerate(val_data):\n",
    "        data_gpu, flow_gt = data\n",
    "        data_gpu = data_gpu.to(device, non_blocking=True) / 255.\n",
    "        flow_gt = flow_gt.to(device, non_blocking=True)\n",
    "        imgs = data_gpu[:, :6]\n",
    "        gt = data_gpu[:, 6:9]\n",
    "        with torch.no_grad():\n",
    "            pred, merged_img, flow, loss_l1, loss_flow, loss_cons, loss_ter, flow_mask = model.update(imgs, gt, training=False)\n",
    "        loss_l1_list.append(loss_l1.cpu().numpy())\n",
    "        loss_flow_list.append(loss_flow.cpu().numpy())\n",
    "        loss_ter_list.append(loss_ter.cpu().numpy())\n",
    "        loss_cons_list.append(loss_cons.cpu().numpy())\n",
    "        for j in range(gt.shape[0]):\n",
    "            psnr = -10 * math.log10(torch.mean((gt[j] - pred[j]) * (gt[j] - pred[j])).cpu().data)\n",
    "            psnr_list.append(psnr)\n",
    "        gt = (gt.permute(0, 2, 3, 1).cpu().numpy() * 255).astype('uint8')\n",
    "        pred = (pred.permute(0, 2, 3, 1).cpu().numpy() * 255).astype('uint8')\n",
    "        merged_img = (merged_img.permute(0, 2, 3, 1).cpu().numpy() * 255).astype('uint8')\n",
    "        flow = flow.permute(0, 2, 3, 1).cpu().numpy()\n",
    "        if i == 0 and local_rank == 0:\n",
    "            for j in range(5):\n",
    "                imgs = np.concatenate((merged_img[i], pred[i], gt[i]), 1)[:, :, ::-1]\n",
    "                writer_val.add_image(str(i) + '/img', imgs.copy(), nr_eval, dataformats='HWC')\n",
    "                writer_val.add_image(str(i) + '/flow', flow2rgb(flow[i][:, :, ::-1]), nr_eval, dataformats='HWC')\n",
    "    \n",
    "    eval_time_interval = time.time() - time_stamp\n",
    "    if local_rank == 0:\n",
    "        print('eval time: {}'.format(eval_time_interval)) \n",
    "        writer_val.add_scalar('loss_l1', np.array(loss_l1_list).mean(), nr_eval)\n",
    "        writer_val.add_scalar('loss_flow', np.array(loss_flow_list).mean(), nr_eval)\n",
    "        writer_val.add_scalar('loss_cons', np.array(loss_cons_list).mean(), nr_eval)\n",
    "        writer_val.add_scalar('loss_ter', np.array(loss_ter_list).mean(), nr_eval)\n",
    "        writer_val.add_scalar('psnr', np.array(psnr_list).mean(), nr_eval)\n",
    "        \n",
    "if __name__ == \"__main__\":    \n",
    "    # parser = argparse.ArgumentParser(description='slomo')\n",
    "    epoch = 300\n",
    "    # parser.add_argument('--batch_size', default=12, type=int, help='minibatch size') # 4 * 12 = 48\n",
    "    batch_size = 12 \n",
    "    # parser.add_argument('--local_rank', default=0, type=int, help='local rank')\n",
    "    local_rank = 0\n",
    "    # parser.add_argument('--world_size', default=4, type=int, help='world size')\n",
    "    world_size = 4\n",
    "    # args = parser.parse_args()\n",
    "    # torch.distributed.init_process_group(backend=\"nccl\", world_size=world_size)\n",
    "    # torch.cuda.set_device(local_rank)\n",
    "    device = torch.device(\"cuda\")\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    model = Model()\n",
    "    train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python373jvsc74a57bd04e68a42444b13db3bacd1cd6c46a066f96ab621132701a69454eeb8e1d76a336",
   "display_name": "Python 3.7.3 64-bit ('pytorchenv': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}